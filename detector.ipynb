{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f619953a",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "  1. [Using the notebook](#using-the-notebook)\n",
    "2. [Guided Example](#guided-example)\n",
    "  1. [Example image selection](#example-image-selection)\n",
    "  2. [Image pre-processing](#image-pre-processing)\n",
    "  3. [Detection and Drawing](#detection-and-drawing)\n",
    "  4. [Widgets](#widgets)\n",
    "3. [Image Upload](#image-upload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a088ab91",
   "metadata": {},
   "source": [
    "# Introduction <a class=\"anchor\" id=\"introduction\"></a>\n",
    "This notebook allows you to upload an image and explore how a machine learning algorithm detects faces within the image. First, I've written a brief guide using an example image that shows how all the significant pieces come together. I've included a few different example images to choose from.\n",
    "\n",
    "The example is followed by a single notebook cell provides the user upload feature. This cell is designed to be able to be used whether or not the example code has be run and therefore contains much of the example code as well.\n",
    "\n",
    "### Using the Notebook <a class=\"anchor\" id=\"using-the-notebook\"></a>\n",
    "To execute a cell, click on it and either press ```Shift + Enter``` or click the 'Run' button in the toolbar above. The cells are meant to be executed sequentially.\n",
    "\n",
    "After a cell is run, the ```In []:``` to the left of the cell should change to ```In [some number]:```.\n",
    "\n",
    "If you want to make an output window larger so you don't have to scroll, single-click in the white margin to the left of the cell.\n",
    "\n",
    "#TODO ADD screenshot here\n",
    "\n",
    "If you get any errors, check to make sure each cell has been executed. Otherwise, try reseting the kernel and running the cells again. To reset the kernel, click ```Kernel > Restart & Clear Output``` in the toolbar above.\n",
    "\n",
    "\n",
    "You can find more information about the notebook environment and how to troubleshoot issues [here](https://nbviewer.jupyter.org/github/ipython/ipython/blob/3.x/examples/Notebook/Index.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c003694",
   "metadata": {},
   "source": [
    "# Guided Example <a class=\"anchor\" id=\"guided-example\"></a>\n",
    "\n",
    "### Example image selection <a class=\"anchor\" id=\"example-image-selection\"></a>\n",
    "\n",
    "There are four images of faces provided with Example 4 being of cartoon faces. All of the provided images are of clear frontal shots of faces as the model chosen here is best suited for this task; models for detecting other objects can be found [here](https://github.com/opencv/opencv/tree/3.4/data).\n",
    "\n",
    "Interestingly, the snippet below serves as a kind of vertical slice since it nicely display the motif that the rest of the code follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06be79e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run this cell first\n",
    "\n",
    "import os\n",
    "import cv2 as cv\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "CASCADE = cv.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "example_selector = widgets.Dropdown(\n",
    "    options=[\n",
    "        ('Select an image...', 0), \n",
    "        ('Example 1', 'test_faces0.jpeg'), \n",
    "        ('Example 2', 'test_faces1.jpeg' ), \n",
    "        ('Example 3', 'test_faces3.jpeg'), \n",
    "        ('Example 4', 'test_faces_cartoon.jpg')\n",
    "    ],\n",
    "    value=0,\n",
    ")\n",
    "widget_output = widgets.Output() # capture widget output for display\n",
    "\n",
    "def handle_selection(change):\n",
    "    with widget_output:\n",
    "        try:\n",
    "            path = os.path.join('test_images', change['new'])\n",
    "            example_img = Image.open(path)\n",
    "            display(example_img)\n",
    "        except TypeError:\n",
    "            print('Please select an example image.')\n",
    "    widget_output.clear_output(wait=True) # clear previous output when new output is displayed\n",
    "    \n",
    "example_selector.observe(handle_selection, names='value')\n",
    "\n",
    "widgets.VBox([widgets.Label('Select an example image'), example_selector, widget_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151ea38e",
   "metadata": {},
   "source": [
    "### Image pre-processing <a class=\"anchor\" id=\"image-pre-processing\"></a>\n",
    "\n",
    "Before much can be done, the example image will need to be converted into a couple differnt formats:\n",
    "\n",
    " - **PIL image object:** PIL is used to both draw on and display the image. While PIL and OpenCV both have drawing capability, I mainly chose to use PIL because it doesn't create a pop-up window to display the image. \n",
    " \n",
    "\n",
    " - **Grayscale image:** The detection model requires a grayscale image format, which is just a 2-D array of 8-bit integers. For this, a 1-D numpy array is created from the image's bytestring, which is in turn converted to the grayscale image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0043fae4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 2 (this cell doesn't have output)\n",
    "\n",
    "EXAMPLE_PATH = os.path.join('test_images', example_selector.value)\n",
    "\n",
    "with open(EXAMPLE_PATH, 'rb') as f:    \n",
    "    img_bytestring = f.read()\n",
    "    \n",
    "pil_img = Image.open(EXAMPLE_PATH) # PIL image object for drawing\n",
    "np_array = np.frombuffer(img_bytestring, dtype=np.uint8) # convert bytestring to 1-D array (intermediate)\n",
    "grayscale = cv.imdecode(np_array, cv.IMREAD_GRAYSCALE) # convert 1-D array to 2-D array for detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969769ea",
   "metadata": {},
   "source": [
    "### Detection and drawing <a class=\"anchor\" id=\"detection-and-drawing\"></a>\n",
    "Now that the image has been processed, faces can be detected. The detection function returns detected faces as (x, y, w, h), where x and y are the coordinates of the top-left corner. From there, boxes can be drawn around on a copy of the image to preserve the original image.\n",
    "\n",
    "Take note of the ```scale_factor``` and ```min_neighbors``` parameters in ```CASCADE.detectMultiScale()```; these parameters allow the sensitivity of the model to be adjusted and will be hooked up to widgets in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fb9cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "\n",
    "def draw_faces(scale_factor, min_neighbors):\n",
    "    detected_faces = CASCADE.detectMultiScale(grayscale, scale_factor, min_neighbors)\n",
    "    \n",
    "    img_copy = pil_img.copy()\n",
    "    draw_context = ImageDraw.Draw(img_copy)\n",
    "    \n",
    "    for face in detected_faces:\n",
    "        draw_context.rectangle((face[0],face[1],face[0]+face[2],face[1]+face[3]), outline='red', width=2)\n",
    "    return img_copy\n",
    "\n",
    "draw_faces(1.05, 3) # use default values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3347043c",
   "metadata": {},
   "source": [
    "### Widgets <a class=\"anchor\" id=\"widgets\"></a>\n",
    "The detection function will need to be re-run everytime the ```scale_factor``` or ```min_neighbors``` widget is changes. This can be done way of event handlers. \n",
    "\n",
    "The Ipywidgets library has a ```widget.observe()``` method that allows a user-defined function to be called every time the widget is used. This observer function mandates that the callable accept a dictionary containing all the information about the widget's changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0128988a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "\n",
    "# widgets\n",
    "scale_slider = widgets.FloatSlider(\n",
    "    value=1.05, \n",
    "    min=1.05,  # must be >=1.01; set to default CASCADE.detectMultiScale() scaleFactor argument\n",
    "    max=1.50,\n",
    "    step=0.05,\n",
    "    description='Scale Factor',\n",
    "    continuous_update=False,  # debounce\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='.2f'\n",
    ")\n",
    "neighbor_slider = widgets.IntSlider(\n",
    "    value=3,  # set default slider value to default CASCADE.detectMultiScale() minNeighbors argument\n",
    "    min=1,\n",
    "    max=10,\n",
    "    step=1,\n",
    "    description='Neighbors',\n",
    "    continuous_update=False,  # debounce\n",
    "    orientation='horizontal',\n",
    "    readout=True\n",
    ")\n",
    "widget_output = widgets.Output() \n",
    "\n",
    "# handler functions for widets\n",
    "def handle_scale_slider(change):\n",
    "    with widget_output:\n",
    "        display(draw_faces(change['new'], neighbor_slider.value))\n",
    "    widget_output.clear_output(wait=True)\n",
    "    \n",
    "def handle_neighbor_slider(change):\n",
    "    with widget_output:  \n",
    "        display(draw_faces(scale_slider.value, change['new']))\n",
    "    widget_output.clear_output(wait=True)\n",
    "    \n",
    "scale_slider.observe(handle_scale_slider, names='value')\n",
    "neighbor_slider.observe(handle_neighbor_slider, names='value')\n",
    "    \n",
    "# display widgets, output\n",
    "display(widgets.VBox([widgets.Label('Upload an image.'),\n",
    "              scale_slider, neighbor_slider, widget_output]))\n",
    "\n",
    "# display example image with no boxes drawn first\n",
    "with widget_output:   \n",
    "    display(pil_img)\n",
    "widget_output.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b35ad52",
   "metadata": {},
   "source": [
    "# Image Upload <a class=\"anchor\" id=\"image-upload\"></a>\n",
    "\n",
    "The cell below allows for detection to be used on an uploaded image.\n",
    "\n",
    "Simply run the cell below and use the upload button to run face detection on an image.\n",
    "\n",
    "The sensitivity of the model can be changed using the ```Scale Factor``` and ```Neighbors``` sliders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc3bcb2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run this cell for the upload feature. \n",
    "\n",
    "import io\n",
    "\n",
    "import cv2 as cv\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "CASCADE = cv.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "uploader = widgets.FileUpload(button_style='info')\n",
    "scale_slider = widgets.FloatSlider(\n",
    "    value=1.05, \n",
    "    min=1.05,  # must be >=1.01; set to default CASCADE.detectMultiScale() scaleFactor argument\n",
    "    max=1.50,\n",
    "    step=0.05,\n",
    "    description='Scale Factor',\n",
    "    continuous_update=False,  # debounce\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='.2f'\n",
    ")\n",
    "neighbor_slider = widgets.IntSlider(\n",
    "    value=3,  # set default slider value to default CASCADE.detectMultiScale() minNeighbors argument\n",
    "    min=1,\n",
    "    max=10,\n",
    "    step=1,\n",
    "    description='Neighbors',\n",
    "    continuous_update=False,  # debounce\n",
    "    orientation='horizontal',\n",
    "    readout=True\n",
    ")\n",
    "widget_output = widgets.Output()\n",
    "\n",
    "def get_upload_formats():\n",
    "    \"\"\"Return PIL and grayscale formats from uploaded image bytestring.\"\"\"\n",
    "    img_bytestring = uploader.data[0]  \n",
    "    pil_img = Image.open(io.BytesIO(img_bytestring))\n",
    "    np_array = np.frombuffer(img_bytestring, dtype=np.uint8)\n",
    "    grayscale = cv.imdecode(np_array, cv.IMREAD_GRAYSCALE)\n",
    "    formats = {\"pil_img\": pil_img, \"grayscale\": grayscale}\n",
    "    return formats\n",
    "\n",
    "def draw_faces(scale_factor, min_neighbors):\n",
    "    \"\"\"Detect faces and return copy of image with boxes drawn around faces.\"\"\"\n",
    "    formats = get_upload_formats()\n",
    "    detected_faces = CASCADE.detectMultiScale(formats['grayscale'], scale_factor, min_neighbors)\n",
    "    \n",
    "    img_copy = formats['pil_img'].copy()\n",
    "    draw_context = ImageDraw.Draw(img_copy)\n",
    "    \n",
    "    for face in detected_faces:\n",
    "        draw_context.rectangle((face[0],face[1],face[0]+face[2],face[1]+face[3]), outline='red', width=2)\n",
    "    return img_copy \n",
    "\n",
    "def handle_scale_slider(change):\n",
    "    \"\"\"Call draws_faces() and display results when scale_factor_slider is changed.\"\"\"\n",
    "    with widget_output:\n",
    "        try: \n",
    "            display(draw_faces(change['new'], neighbor_slider.value))\n",
    "        except IndexError: # handle slider movement w/o upload\n",
    "            display('Please upload an image first.')\n",
    "    widget_output.clear_output(wait=True)\n",
    "    \n",
    "def handle_neighbor_slider(change):\n",
    "    \"\"\"Call draws_faces() and display results when min_neighbors_slider is changed.\"\"\"\n",
    "    with widget_output:\n",
    "        try:      \n",
    "            display(draw_faces(scale_slider.value, change['new']))\n",
    "        except IndexError: # handle slider movement w/o upload\n",
    "            display('Please upload an image first.')\n",
    "    widget_output.clear_output(wait=True)\n",
    "    \n",
    "def handle_upload(change):\n",
    "    \"\"\"Call draws_faces() and display results when image is uploaded.\"\"\"\n",
    "    scale_slider.value = 1.05\n",
    "    neighbor_slider.value = 3\n",
    "    \n",
    "    with widget_output:\n",
    "        try: \n",
    "            display(draw_faces(scale_slider.value, neighbor_slider.value))\n",
    "        except Image.UnidentifiedImageError: # handle upload of non-image file\n",
    "            display(\"Please upload an image file.\")\n",
    "    widget_output.clear_output(wait=True)\n",
    "      \n",
    "uploader.observe(handle_upload, names='_counter')\n",
    "scale_slider.observe(handle_scale_slider, names='value')\n",
    "neighbor_slider.observe(handle_neighbor_slider, names='value')\n",
    "\n",
    "widgets.VBox([widgets.Label('Upload an image.'),\n",
    "              uploader, scale_slider, neighbor_slider, widget_output])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
